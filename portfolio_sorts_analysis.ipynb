{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib inline plotting\n",
    "%matplotlib inline\n",
    "# make inline plotting higher resolution\n",
    "%config InlineBackend.figure_format ='svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gmean\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helpers.portfolio_sorts import estimate_portfolio_sorts, last_of_month\n",
    "from helpers.expand_to_daily import expand_to_daily\n",
    "from helpers.hml import high_minus_low, construct_portfolio\n",
    "from helpers.sql import connect_to_db, read_db, vacuum_db, update_database\n",
    "from helpers.pretty_print import pretty_print_pval\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we import the results of the portfolio sort analysis\n",
    "\n",
    "Remember that we only do the sorts for one type of the sentiment score (we can potentially vary this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('new_pf_sorts_nw7_sum_sentiment.bin', 'rb') as file:\n",
    "# with open('new_pf_sorts_nw7_mean_sentiment.bin', 'rb') as file:\n",
    "    # read information from portfolio sorts results\n",
    "    estimated_portfolio_sorts = pickle.load(file)\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = connect_to_db()\n",
    "\n",
    "# setting newey west lags to 7 according to Greene (Econometric Analysis, 7th edition, section 20.5.2, p. 960)\n",
    "# note this is different than the setting in `portfolio_sorts.ipynb`\n",
    "NW_LAGS: int = 7\n",
    "\n",
    "# define looking-back period (number of months)\n",
    "LOOK_BACK: int = 3\n",
    "\n",
    "# number of obs in a year\n",
    "YEARLY_BUSINESS_DAYS: int = 250\n",
    "\n",
    "# set significant digits\n",
    "SIGNIFICANT_DIGITS: int = 4\n",
    "\n",
    "# define important dates\n",
    "START_DATE_MISSING = datetime(year=2010, month=1, day=1)\n",
    "\n",
    "# Actual data-ranges\n",
    "START_DATE = datetime(year=2010, month=1, day=4)\n",
    "END_DATE = datetime(year=2023, month=1, day=1)\n",
    "\n",
    "# Models\n",
    "AP_MODELS = {\n",
    "    \"CAPM\": [\"mkt-rf\"],\n",
    "    \"FF3\": [\"mkt-rf\", \"smb\", \"hml\"],\n",
    "    \"FF3_C\": [\"mkt-rf\", \"smb\", \"hml\", \"mom\"],\n",
    "    \"FF5\": [\"mkt-rf\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n",
    "}\n",
    "\n",
    "# Select sentiment measures \n",
    "SENTMENT_MEASURES = [\n",
    "    \"aggregate_transformed_residuals\",\n",
    "    \"politics_transformed_residuals\",\n",
    "    \"importance_of_human_intervantion_transformed_residuals\",\n",
    "    \"weather_extremes_transformed_residuals\",\n",
    "]\n",
    "\n",
    "# Splits\n",
    "SPLITS = [0.1, 0.2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch series of business days (will be used to clean E portfolio and factors)\n",
    "business_days = read_db(engine=engine, statement='select \"index\", date from returns')\n",
    "\n",
    "business_days = business_days.set_index(\"date\", drop=True).index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "meta = read_db(\n",
    "    statement=\"select * from meta\", engine=engine\n",
    ")\n",
    "meta = meta.set_index('instrument')\n",
    "meta.index.name = None\n",
    "\n",
    "environment = read_db(\n",
    "    engine=engine, statement='select * from environment', idx_col='date')\n",
    "\n",
    "emissions = read_db(\n",
    "    engine=engine, statement='select * from emissions', idx_col='date')\n",
    "\n",
    "sentiment = read_db(\n",
    "    statement=\"select * from climate_sum_ar1\", engine=engine, idx_col=\"date\"\n",
    ")\n",
    "\n",
    "market_cap = read_db(\n",
    "    engine=engine, statement=\"select * from market_cap\", idx_col=\"date\"\n",
    ")\n",
    "\n",
    "returns = read_db(\n",
    "    engine=engine, statement=\"select * from returns\", idx_col=\"date\"\n",
    ")\n",
    "\n",
    "snp500 = read_db(\n",
    "    engine=engine, statement=\"select * from snp\", idx_col=\"date\"\n",
    ").sort_index()\n",
    "\n",
    "factors = read_db(\n",
    "    engine=engine, statement=\"select * from factors\", idx_col=\"date\"\n",
    ")\n",
    "\n",
    "riskfree = read_db(\n",
    "    engine=engine, statement=\"select * from riskfree\", idx_col=\"date\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate excess returns r_{i}-r_{rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate excess returns\n",
    "returns = pd.merge(left=returns, right=riskfree, how='left', right_index=True, left_index=True)\n",
    "\n",
    "returns.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in returns.columns:\n",
    "    if col == \"rf\":\n",
    "        continue\n",
    "\n",
    "    returns[col] = returns[col] - returns[\"rf\"]\n",
    "\n",
    "returns = returns.drop(columns=[\"rf\"])\n",
    "\n",
    "returns.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up S&P data. Here we remove all S&P observations that we\n",
    "# do NOT have observations for\n",
    "snp500 = snp500[snp500[\"ric\"].isin(returns.columns)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting factor space $X_t$\n",
    "factors = factors.loc[START_DATE:]\n",
    "\n",
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand to daily for market cap. Fill between but don't forward fill\n",
    "market_cap = expand_to_daily(\n",
    "    market_cap,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    start_date_missing=START_DATE_MISSING,\n",
    "    last_valid=True,\n",
    ")\n",
    "\n",
    "# expand to daily for stock-returns\n",
    "returns = expand_to_daily(\n",
    "    returns,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    start_date_missing=START_DATE_MISSING,\n",
    "    ffill=True,\n",
    "    cut_sample=True,\n",
    "    last_valid=True,\n",
    "    ffill_limit=None,\n",
    ")\n",
    "riskfree = expand_to_daily(\n",
    "    riskfree,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    start_date_missing=START_DATE_MISSING,\n",
    "    ffill=True,\n",
    "    cut_sample=True,\n",
    "    last_valid=True,\n",
    "    ffill_limit=None,\n",
    ")\n",
    "\n",
    "# expand to daily for ESG scores. Fill one year ahead unless...\n",
    "environment = expand_to_daily(\n",
    "    environment,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    start_date_missing=START_DATE_MISSING,\n",
    "    ffill_limit=365,\n",
    ")\n",
    "# we also choose to back-fill ESG data due to limited data in the beginning of the sample\n",
    "environment = environment.bfill()\n",
    "\n",
    "emissions = expand_to_daily(\n",
    "    emissions,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    start_date_missing=START_DATE_MISSING,\n",
    "    ffill_limit=365,\n",
    ")\n",
    "emissions = emissions.bfill()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# drop columns that are all NaN\n",
    "market_cap = market_cap.dropna(how=\"all\", axis=1)\n",
    "returns = returns.dropna(how=\"all\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Portfolio sort test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_sort_test(\n",
    "    hml: pd.DataFrame,\n",
    "    hml_name: str,\n",
    "    split_forward: bool = False,\n",
    "    split_backward: bool = False,\n",
    "):\n",
    "    # Get model type\n",
    "    model = hml_name.split(\":\")[1]\n",
    "\n",
    "    Y = hml.groupby(\"date\").first()[\"hml_return\"]\n",
    "    Y = Y.reindex(business_days).dropna()\n",
    "\n",
    "    # select subsample\n",
    "    if split_forward:\n",
    "        Y = Y.loc[\"2014-01-01\":]\n",
    "    if split_backward:\n",
    "        Y = Y.loc[:\"2014-01-01\"]\n",
    "\n",
    "\n",
    "    X = factors[AP_MODELS[model]]\n",
    "    X = sm.add_constant(X)\n",
    "    X = X.reindex(Y.index).dropna()\n",
    "\n",
    "    # estimate time-series regression using Newey West errors\n",
    "    fit = sm.OLS(endog=Y, exog=X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": NW_LAGS})\n",
    "\n",
    "    arrays = [\n",
    "        np.repeat(hml_name.split(\":\")[1], 3),\n",
    "        np.repeat(hml_name.split(\":\")[0], 3),\n",
    "        [\"annual_return\", \"annual_alpha\", \"alpha_t\"],\n",
    "    ]\n",
    "    tuples = list(zip(*arrays))\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "        [\n",
    "            Y.mean() * YEARLY_BUSINESS_DAYS,\n",
    "            fit.params[\"const\"] * YEARLY_BUSINESS_DAYS,\n",
    "            fit.tvalues[\"const\"],\n",
    "        ],\n",
    "        index=index,\n",
    "        columns=[hml_name.split(\":\")[2]],\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_results = []\n",
    "\n",
    "for sort in estimated_portfolio_sorts.keys():\n",
    "    # run on split\n",
    "    sort_results.append(portfolio_sort_test(estimated_portfolio_sorts[sort], sort, split_forward=False, split_backward=False))\n",
    "    # sort_results.append(portfolio_sort_test(estimated_portfolio_sorts[sort], sort, split_forward=True, split_backward=False))\n",
    "    # sort_results.append(portfolio_sort_test(estimated_portfolio_sorts[sort], sort, split_forward=False, split_backward=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "sort_results_joined = []\n",
    "\n",
    "for group in chunker(sort_results, len(SENTMENT_MEASURES)):\n",
    "    \n",
    "    placeholder = pd.DataFrame(index=group[0].index)\n",
    "\n",
    "    for j in group:\n",
    "        placeholder = pd.merge(placeholder, j, left_index=True, right_index=True)\n",
    "    \n",
    "    sort_results_joined.append(placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_results_concat = pd.DataFrame()\n",
    "\n",
    "for frame in sort_results_joined:\n",
    "    sort_results_concat = pd.concat([sort_results_concat, frame])\n",
    "\n",
    "\n",
    "sort_results_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in sort_results_concat.loc[\n",
    "    sort_results_concat.index.get_level_values(2).str.contains(\"alpha_t\")\n",
    "].index:\n",
    "    \n",
    "    sort_results_concat.loc[row] = (\n",
    "        sort_results_concat.loc[row]\n",
    "        .apply(\n",
    "            pretty_print_pval,\n",
    "            freedom=estimated_portfolio_sorts[\n",
    "                \"0.1:CAPM:aggregate_transformed_residuals\"\n",
    "            ][\"date\"]\n",
    "            .unique()\n",
    "            .shape[0],\n",
    "            precision=SIGNIFICANT_DIGITS,\n",
    "        )\n",
    "        .astype(str)\n",
    "        .mask(sort_results_concat.loc[row].isna())\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename and prettify\n",
    "sort_results_concat.index = sort_results_concat.index.map(lambda x: (x[0], x[1].replace('0.1', 'Deciles'), x[2]))\n",
    "sort_results_concat.index = sort_results_concat.index.map(lambda x: (x[0], x[1].replace('0.2', 'Quintiles'), x[2]))\n",
    "\n",
    "\n",
    "sort_results_concat.index = sort_results_concat.index.map(lambda x: (x[0], x[1], x[2].replace('annual_return', '$\\\\mathbb{E}\\\\left[r_{t}^{UCP}\\\\right]$')))\n",
    "sort_results_concat.index = sort_results_concat.index.map(lambda x: (x[0], x[1], x[2].replace('annual_alpha', '$\\\\widehat{\\\\alpha}$')))\n",
    "sort_results_concat.index = sort_results_concat.index.map(lambda x: (x[0], x[1], x[2].replace('alpha_t', '$t\\\\left(\\\\widehat{\\\\alpha}\\\\right)$')))\n",
    "\n",
    "sort_results_concat.columns = sort_results_concat.columns.map(lambda x: x.replace('_transformed_residuals', ''))\n",
    "sort_results_concat.columns = sort_results_concat.columns.map(lambda x: x.replace('_', ' '))\n",
    "sort_results_concat.columns = sort_results_concat.columns.map(lambda x: x.title())\n",
    "\n",
    "\n",
    "sort_results_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = sort_results_concat.to_latex(\n",
    "    index=True,\n",
    "    escape=False,\n",
    "    sparsify=True,\n",
    "    multirow=True,\n",
    "    multicolumn=True,\n",
    "    bold_rows=True,\n",
    "    multicolumn_format=\"c\",\n",
    "    float_format=f\"{{:.{SIGNIFICANT_DIGITS}f}}\".format,\n",
    "    position=\"H\",\n",
    ")\n",
    "\n",
    "latex = re.sub(r\"\\\\(mid|top|bottom)rule\", \"\", latex)\n",
    "\n",
    "print(latex)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting average loadings\n",
    "\n",
    "Plot the time-series of the average loadings within each estimate for top and bottom quantiles.\n",
    "\n",
    "This is simply a resource to do some descriptive on the data. We will probably need some of this to work futher on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = (\n",
    "    estimated_portfolio_sorts[\"0.1:CAPM:aggregate_transformed_residuals\"]\n",
    "    .groupby([\"date\", \"hml\"])[\"rating\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .set_index(\"date\")\n",
    ")\n",
    "\n",
    "group = group.loc[group[\"hml\"].isin([True, False])]\n",
    "\n",
    "group = group[[\"hml\", \"rating\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "group.loc[group[\"hml\"] == True].plot(ax=ax, label=\"Top 20% mean\")\n",
    "group.loc[group[\"hml\"] == False].plot(ax=ax, label=\"Top 20% mean\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot returns for top and bottom groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    \"0.1\": \"Deciles\",\n",
    "    \"0.2\": \"Quintiles\",\n",
    "    \"aggregate_transformed_residuals\": \"Aggregate\",\n",
    "    \"politics_transformed_residuals\": \"Politics\",\n",
    "    \"weather_extremes_transformed_residuals\": \"Weather Extremes\",\n",
    "    \"importance_of_human_intervantion_transformed_residuals\": \"Imp. of Human Intervention\",\n",
    "    \n",
    "    \"CAPM\": \"CAPM\",\n",
    "    \"FF3_C\": \"FFC\",\n",
    "    \"FF3\": \"FF3\",\n",
    "    \"FF5\": \"FF5\",\n",
    "\n",
    "    \"bottom\": \"Bottom\",\n",
    "    \"top\": \"Top\",\n",
    "}\n",
    "\n",
    "\n",
    "def plot_returns(sorted_dict: dict, decile: bool = True, title: str = None):\n",
    "    if decile:\n",
    "        sorted_dict_filtered = {\n",
    "            key: sorted_dict[key] for key in sorted_dict if re.match(\"0.1\", key)\n",
    "        }\n",
    "\n",
    "    if not decile:\n",
    "        sorted_dict_filtered = {\n",
    "            key: sorted_dict[key] for key in sorted_dict if re.match(\"0.2\", key)\n",
    "        }\n",
    "\n",
    "    # overwrite object\n",
    "    sorted_dict = sorted_dict_filtered\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(13, 12), sharex=True, sharey=True)\n",
    "\n",
    "    for i, (key, df) in enumerate(sorted_dict.items()):\n",
    "        Y = df.groupby(\"date\").first()[[\"high_return\", \"low_return\"]]\n",
    "\n",
    "        # make sure to drop non-business days\n",
    "        Y = Y.reindex(business_days).dropna()\n",
    "\n",
    "        # rename columns\n",
    "        if decile:\n",
    "            Y = Y.rename(columns={\"high_return\": \"Top 10%\", \"low_return\": \"Bottom 10%\"})\n",
    "        else:\n",
    "            Y = Y.rename(columns={\"high_return\": \"Top 20%\", \"low_return\": \"Bottom 20%\"})\n",
    "\n",
    "        # join riskfree rate\n",
    "        Y = pd.merge(left=Y, right=riskfree, left_index=True, right_index=True)\n",
    "        for col in Y:\n",
    "            if col != \"rf\":\n",
    "                Y[col] = Y[col] - Y[\"rf\"]\n",
    "        Y = Y.drop(columns=[\"rf\"])\n",
    "\n",
    "        # plot excess returns\n",
    "        ((Y + 1).cumprod() - 1).plot(\n",
    "            ax=axes[i // 4, i % 4],\n",
    "            legend=False,\n",
    "            style=[\"-\", \"-\"],\n",
    "            color=[\"C3\", \"C0\"],\n",
    "        )\n",
    "\n",
    "        axes[i // 4, i % 4].set_ylabel(\"Cumulative (excess) return (%)\", fontsize=9)\n",
    "\n",
    "        axes[i // 4, i % 4].set_title(\n",
    "            f'{names[key.split(\":\")[2]]} ({names[key.split(\":\")[1]]})',\n",
    "            fontdict={\"fontsize\": 11},\n",
    "        )\n",
    "\n",
    "        # break\n",
    "\n",
    "    # Add title\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # add legend\n",
    "    plt.figlegend(\n",
    "        loc=\"lower center\",\n",
    "        fontsize=12,\n",
    "        ncol=2,\n",
    "        labelspacing=1,\n",
    "        bbox_to_anchor=(0.5, -0.04),\n",
    "        handles=axes[0, 0].get_legend_handles_labels()[0],\n",
    "    )\n",
    "\n",
    "    # adjust spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if decile:\n",
    "        fig.savefig(f\"plots/deciles_pf_sorts.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        fig.savefig(f\"plots/quintiles_pf_sorts.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "plot_returns(estimated_portfolio_sorts, decile=False, title=\"Cumulative excess returns for portfolios sort (quintile breakpoints)\")\n",
    "plot_returns(estimated_portfolio_sorts, decile=True, title=\"Cumulative excess returns for portfolios sort (decile breakpoints)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform environment from TS to panel and store in `env_panel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_panel(df: pd.DataFrame, col_name: str, frequency: str = \"D\") -> pd.DataFrame:\n",
    "    output = pd.DataFrame(columns=[\"date\", \"snp_ric\", col_name])\n",
    "\n",
    "    for col in tqdm(df.columns):\n",
    "        series = df[col].copy()\n",
    "        series = series.reset_index()\n",
    "\n",
    "        series[\"snp_ric\"] = col\n",
    "        series = series.rename(columns={\"index\": \"date\", col: col_name})\n",
    "\n",
    "        output = pd.concat([output, series], axis=0)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "emi_panel = to_panel(emissions, col_name=\"emissions\", frequency=\"D\")\n",
    "env_panel = to_panel(environment, col_name=\"env\", frequency=\"D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorts = {}\n",
    "\n",
    "for key, df in tqdm(\n",
    "    estimated_portfolio_sorts.items(), total=len(estimated_portfolio_sorts)\n",
    "):\n",
    "    top_tickers = df.loc[(df[\"hml\"] == True)]\n",
    "    bottom_tickers = df.loc[~(df[\"hml\"] == True)]\n",
    "\n",
    "    # join E rating\n",
    "    top_tickers = pd.merge(\n",
    "        left=env_panel,\n",
    "        right=top_tickers,\n",
    "        left_on=[\"date\", \"snp_ric\"],\n",
    "        right_on=[\"date\", \"snp_ric\"],\n",
    "    )\n",
    "    bottom_tickers = pd.merge(\n",
    "        left=env_panel,\n",
    "        right=bottom_tickers,\n",
    "        left_on=[\"date\", \"snp_ric\"],\n",
    "        right_on=[\"date\", \"snp_ric\"],\n",
    "    )\n",
    "\n",
    "    # join emissions\n",
    "    top_tickers = pd.merge(\n",
    "        left=emi_panel,\n",
    "        right=top_tickers,\n",
    "        left_on=[\"date\", \"snp_ric\"],\n",
    "        right_on=[\"date\", \"snp_ric\"],\n",
    "    )\n",
    "    bottom_tickers = pd.merge(\n",
    "        left=emi_panel,\n",
    "        right=bottom_tickers,\n",
    "        left_on=[\"date\", \"snp_ric\"],\n",
    "        right_on=[\"date\", \"snp_ric\"],\n",
    "    )\n",
    "\n",
    "    # join meta data\n",
    "    top_tickers = pd.merge(\n",
    "        left=top_tickers,\n",
    "        right=meta[[\"company_common_name\", \"trbc_industry_name\"]],\n",
    "        left_on=[\"snp_ric\"],\n",
    "        right_index=True,\n",
    "    )\n",
    "    bottom_tickers = pd.merge(\n",
    "        left=bottom_tickers,\n",
    "        right=meta[[\"company_common_name\", \"trbc_industry_name\"]],\n",
    "        left_on=[\"snp_ric\"],\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    sorts[f\"top:{key}\"] = top_tickers.sort_values('date')\n",
    "    sorts[f\"bottom:{key}\"] = bottom_tickers.sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_models = [\n",
    "    '0.1:CAPM:aggregate_transformed_residuals',\n",
    "    '0.1:CAPM:politics_transformed_residuals',\n",
    "    '0.1:CAPM:importance_of_human_intervantion_transformed_residuals',\n",
    "    '0.2:CAPM:aggregate_transformed_residuals',\n",
    "    '0.2:CAPM:politics_transformed_residuals',\n",
    "    '0.2:CAPM:importance_of_human_intervantion_transformed_residuals',\n",
    "\n",
    "    '0.1:FF3:aggregate_transformed_residuals',\n",
    "    '0.1:FF3:politics_transformed_residuals',\n",
    "    '0.1:FF3:importance_of_human_intervantion_transformed_residuals',\n",
    "    '0.2:FF3:aggregate_transformed_residuals',\n",
    "    '0.2:FF3:politics_transformed_residuals',\n",
    "    '0.2:FF3:importance_of_human_intervantion_transformed_residuals',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorts['top:0.2:CAPM:importance_of_human_intervantion_transformed_residuals'].groupby('date')['emissions'].median().plot()\n",
    "sorts['bottom:0.2:CAPM:importance_of_human_intervantion_transformed_residuals'].groupby('date')['emissions'].median().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ratings = {}\n",
    "\n",
    "for sig in significant_models:\n",
    "    for hml in ['top', 'bottom']:\n",
    "        df = sorts[f'{hml}:{sig}']\n",
    "\n",
    "        env_ratings[f'{hml}:{sig}'] = {\n",
    "            'e_rating:mean': df.groupby('date')['env'].mean().mean(),\n",
    "            'e_rating:std':df.groupby('date')['env'].mean().std(), \n",
    "            'emissions:mean': df.groupby('date')['emissions'].mean().mean(),\n",
    "            'emissions:std': df.groupby('date')['emissions'].mean().std(),\n",
    "            'emissions:coverage': 1 - (df.set_index('date').isnull().groupby('date')['emissions'].sum() / df.groupby('date')['emissions'].count()).mean(),\n",
    "            #'emissions:median': df.groupby('date')['emissions'].median().mean(),\n",
    "        }\n",
    "\n",
    "\n",
    "env_ratings = pd.DataFrame(env_ratings).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ratings['name'] = env_ratings.index\n",
    "\n",
    "\n",
    "index_cols = ['sort', 'decile', 'model', 'sent']\n",
    "env_ratings[index_cols] = env_ratings['name'].str.split(':', expand=True)\n",
    "\n",
    "for col in index_cols:\n",
    "    env_ratings[col] = env_ratings[col].map(names)\n",
    "\n",
    "env_ratings = env_ratings.set_index(['model', 'decile', 'sent', 'sort'])\n",
    "env_ratings = env_ratings.drop(columns='name')\n",
    "\n",
    "env_ratings.index.name = None\n",
    "\n",
    "\n",
    "# format coverage as %\n",
    "env_ratings['emissions:coverage'] = env_ratings['emissions:coverage'].apply(lambda x: f'{x:.0%}')\n",
    "\n",
    "#env_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ratings = env_ratings.transpose()\n",
    "env_ratings['names'] = env_ratings.index\n",
    "env_ratings['model'] = env_ratings['names'].str.split(':', expand=True)[0]\n",
    "env_ratings['stat'] = env_ratings['names'].str.split(':', expand=True)[1]\n",
    "env_ratings = env_ratings.set_index(['model', 'stat'], drop=True)\n",
    "env_ratings = env_ratings.drop(columns='names')\n",
    "env_ratings = env_ratings.transpose()\n",
    "env_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = env_ratings.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    sparsify=True,\n",
    "    multirow=True,\n",
    "    multicolumn=True,\n",
    "    bold_rows=True,\n",
    "    multicolumn_format=\"c\",\n",
    "    float_format=\"{:.2f}\".format,\n",
    "    position=\"H\",\n",
    ")\n",
    "\n",
    "latex = re.sub(r\"\\\\(mid|top|bottom)rule\", \"\", latex)\n",
    "\n",
    "print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_bottom(df):\n",
    "    top10 = (\n",
    "        df[\"snp_ric\"]\n",
    "        .to_frame()\n",
    "        .apply(pd.Series.value_counts)\n",
    "        .sort_values(by=\"snp_ric\", ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    top10 = pd.merge(\n",
    "        left=top10,\n",
    "        right=df.groupby(\"snp_ric\")[\"env\"].mean().to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    top10 = pd.merge(\n",
    "        left=top10,\n",
    "        right=df.groupby(\"snp_ric\")[\"emissions\"].mean().to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    top10 = pd.merge(\n",
    "        left=top10,\n",
    "        right=meta[[\"company_common_name\", \"trbc_industry_name\"]],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    top10 = top10.rename(\n",
    "        columns={\n",
    "            \"env\": \"Mean E Rating\",\n",
    "            \"snp_ric\": \"Occurrences\",\n",
    "            \"company_common_name\": \"Company Name\",\n",
    "            \"trbc_industry_name\": \"Industry\",\n",
    "            'emissions': 'CO2'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # move index\n",
    "    top10[\"Ticker\"] = top10.index\n",
    "    top10 = top10.reset_index(drop=True)\n",
    "\n",
    "    top10 = top10[\n",
    "        [\"Ticker\", \"Company Name\", \"Industry\", \"Occurrences\", \"Mean E Rating\", 'CO2']\n",
    "    ]\n",
    "\n",
    "    return top10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of human..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_sorts = top_bottom(sorts[\"bottom:0.2:CAPM:importance_of_human_intervantion_transformed_residuals\"].copy())\n",
    "bottom_10_sorts = top_bottom(sorts[\"top:0.2:CAPM:importance_of_human_intervantion_transformed_residuals\"].copy())\n",
    "\n",
    "\n",
    "top_10_sorts['Sort'] = 'Top 10'\n",
    "bottom_10_sorts['Sort'] = 'Bottom 10'\n",
    "\n",
    "summary_importance = pd.concat([top_10_sorts, bottom_10_sorts])\n",
    "\n",
    "summary_importance = summary_importance.set_index('Sort')\n",
    "\n",
    "summary_importance = summary_importance.drop(columns=['Ticker'])\n",
    "\n",
    "summary_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = summary_importance.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    sparsify=True,\n",
    "    multirow=True,\n",
    "    multicolumn=True,\n",
    "    bold_rows=True,\n",
    "    multicolumn_format=\"c\",\n",
    "    float_format=\"{:.2f}\".format,\n",
    "    position=\"H\",\n",
    ")\n",
    "\n",
    "latex = re.sub(r\"\\\\(mid|top|bottom)rule\", \"\", latex)\n",
    "\n",
    "print(latex)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Politics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_sorts = top_bottom(sorts[\"bottom:0.2:FF3:politics_transformed_residuals\"].copy())\n",
    "bottom_10_sorts = top_bottom(sorts[\"top:0.2:FF3:politics_transformed_residuals\"].copy())\n",
    "\n",
    "\n",
    "top_10_sorts['Sort'] = 'Top 10'\n",
    "bottom_10_sorts['Sort'] = 'Bottom 10'\n",
    "\n",
    "summary_pol = pd.concat([top_10_sorts, bottom_10_sorts])\n",
    "\n",
    "summary_pol = summary_pol.set_index('Sort')\n",
    "\n",
    "summary_pol = summary_pol.drop(columns=['Ticker'])\n",
    "\n",
    "summary_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = summary_pol.loc[summary_pol['Company Name'] != 'Southern Co'].dropna()['CO2'].groupby('Sort').mean()\n",
    "\n",
    "std = summary_pol.loc[summary_pol['Company Name'] != 'Southern Co'].dropna()['CO2'].groupby('Sort').std()\n",
    "\n",
    "display(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = summary_pol.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    sparsify=True,\n",
    "    multirow=True,\n",
    "    multicolumn=True,\n",
    "    bold_rows=True,\n",
    "    multicolumn_format=\"c\",\n",
    "    float_format=\"{:.2f}\".format,\n",
    "    position=\"H\",\n",
    ")\n",
    "\n",
    "latex = re.sub(r\"\\\\(mid|top|bottom)rule\", \"\", latex)\n",
    "\n",
    "print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hml_corr = factors['e-hml'].dropna().to_frame()\n",
    "\n",
    "for key, df in estimated_portfolio_sorts.items():\n",
    "    if key not in significant_models:\n",
    "        continue\n",
    "\n",
    "    #display(key)\n",
    "    #display(df.groupby('date')['hml_return'].first())\n",
    "    hml_corr = pd.merge(left=hml_corr, right=df.groupby('date')['hml_return'].first(), left_index=True, right_index=True)\n",
    "    hml_corr = hml_corr.rename(columns={'hml_return': key})\n",
    "\n",
    "\n",
    "hml_corr = hml_corr.transpose()\n",
    "\n",
    "hml_corr['names'] = hml_corr.index\n",
    "\n",
    "hml_corr['sort'] = [names.get(i) for i in hml_corr['names'].str.split(':', expand=True)[0]]\n",
    "hml_corr['model'] = [names.get(i) for i in hml_corr['names'].str.split(':', expand=True)[1]]\n",
    "hml_corr['sent'] = [names.get(i) for i in hml_corr['names'].str.split(':', expand=True)[2]]\n",
    "\n",
    "hml_corr[['sort', 'model', 'sent']] = hml_corr[['sort', 'model', 'sent']].fillna('e-hml')\n",
    "hml_corr = hml_corr.set_index(['model', 'sort', 'sent'])\n",
    "\n",
    "hml_corr = hml_corr.drop(columns='names').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(hml_corr.corr()))\n",
    "np.fill_diagonal(mask, 0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "\n",
    "sns.heatmap(hml_corr.corr(), annot=True, fmt=\".2f\", ax=ax, linewidths=0.5, mask=mask)\n",
    "ax.set(xlabel=\"\", ylabel=\"\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels[0] = 'e-hml'\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "plt.savefig(f\"plots/correlation_matrix.png\", dpi=450, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AJA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
